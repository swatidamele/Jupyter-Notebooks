{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "ipy-jupyter-venv3",
      "language": "python",
      "name": "myipy_jupter_env3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "name": "Naive_Bayes.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swatidamele/Jupyter-Notebooks/blob/main/Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03e9758b"
      },
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "from random import randrange\n",
        "import random"
      ],
      "id": "03e9758b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0381161d"
      },
      "source": [
        "NEG = 0\n",
        "POS = 1\n",
        "ALL = 2"
      ],
      "id": "0381161d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f459219",
        "outputId": "80667f2a-73cd-488d-fa27-8ee209a6686b"
      },
      "source": [
        "df=pd.read_csv(r\"../input/sentiment-labelled-sentences-data-set/sentiment labelled sentences/amazon_cells_labelled.txt\",delimiter=\"\\t\",header=None,names=[\"IMDB Review\",\"Sentiment\"])\n",
        "df.columns"
      ],
      "id": "4f459219",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../input/sentiment-labelled-sentences-data-set/sentiment labelled sentences/amazon_cells_labelled.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13008-bb9f6f036a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"../input/sentiment-labelled-sentences-data-set/sentiment labelled sentences/amazon_cells_labelled.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IMDB Review\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/ipy-jupyter-venv3/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/ipy-jupyter-venv3/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/ipy-jupyter-venv3/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/ipy-jupyter-venv3/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/ipy-jupyter-venv3/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/ipy-jupyter-venv3/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/ipy-jupyter-venv3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/sentiment-labelled-sentences-data-set/sentiment labelled sentences/amazon_cells_labelled.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7d7f17e"
      },
      "source": [
        "df=df.replace(',','',regex=True).replace('!','',regex=True).replace('-','',regex=True).replace('.','',regex=True)\n",
        "df    "
      ],
      "id": "b7d7f17e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff779bef"
      },
      "source": [
        "split_1 = int(0.8 * len(df))\n",
        "split_2 = int(0.9 * len(df))\n",
        "train_data = df[:split_1]\n",
        "dev_data = df[split_1:split_2]\n",
        "test_data = df[split_2:]\n",
        "print(\"Train Data\")\n",
        "print((train_data))\n",
        "print(\"Dev Data\")\n",
        "print((dev_data))\n",
        "print(\"Test Data\")\n",
        "print((test_data))"
      ],
      "id": "ff779bef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2e4dfb5"
      },
      "source": [
        "# rawData = open('imdb_labelled.txt').read()"
      ],
      "id": "c2e4dfb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfaaefbf"
      },
      "source": [
        "# l = ['.',',','!','(',')','-','[',']','{','}',';',':','<','>','/','?','@','#','$','%','^','&','*','_','~']\n",
        "# validation_data = [ x.replace(y,'')  for x in validation_data_temp for y in l if y in x ]\n",
        "# print(validation_data)\n",
        "\n",
        "                       "
      ],
      "id": "cfaaefbf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ef91630"
      },
      "source": [
        "validation_data=train_data.values.tolist()\n"
      ],
      "id": "7ef91630",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "975391de"
      },
      "source": [
        "def count_label(data):\n",
        "    label_count=[0,0, len(data)] # [neg_count, pos_count, all_count]\n",
        "    for item in data:\n",
        "        #label_count[ALL]+=1\n",
        "        if item[1]== 0:\n",
        "            label_count[NEG]+=1\n",
        "        else:\n",
        "            label_count[POS]+=1\n",
        "            \n",
        "    return label_count"
      ],
      "id": "975391de",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdfc8c3a"
      },
      "source": [
        "def word_vocabulary(data,threshold):\n",
        "   \n",
        "    vocab={}\n",
        "    \n",
        "    for sent in data:\n",
        "        count=0\n",
        "        for content in sent:\n",
        "            if(count%2==0): \n",
        "                word_list = content.split()\n",
        "                #print(word_list+\"\\n\")\n",
        "                for word in word_list:\n",
        "                    #print(word)\n",
        "                    if word.lower() not in vocab:\n",
        "                        vocab[word.lower()]=[0,0,0]\n",
        "                    else:\n",
        "                        vocab[word.lower()][ALL]+=1\n",
        "                        #print(vocab[word.lower()][POS])\n",
        "                        if(sent[count+1]==1):\n",
        "                            vocab[word.lower()][POS]+=1\n",
        "                            #print(\"testPOS\")\n",
        "                        else:\n",
        "                            if(sent[count+1]==0):\n",
        "                                vocab[word.lower()][NEG]+=1\n",
        "                                #print(\"testPOS\")\n",
        "                count+=1\n",
        "                    \n",
        "    for key in list(vocab.keys()):\n",
        "        #print(key)\n",
        "        if vocab[key][ALL]<threshold:\n",
        "            del vocab[key]\n",
        "            \n",
        "    #print(vocab.values())            \n",
        "    return(vocab)"
      ],
      "id": "bdfc8c3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ce0456d"
      },
      "source": [
        "total_count = count_label(validation_data)\n",
        "print(\"Count of sentences [Negative,Positive,All] is \",total_count)"
      ],
      "id": "0ce0456d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a8ecb82"
      },
      "source": [
        "Dict = word_vocabulary(validation_data,5)\n",
        "df_dict = pd.DataFrame(list(Dict.items()),columns = ['Word','Count [Negative,Positive,All]'])  \n",
        "print(df_dict)"
      ],
      "id": "4a8ecb82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dac104b"
      },
      "source": [
        "P_word = {}\n",
        "\n",
        "for word in Dict:\n",
        "    word_count = int(Dict[word][ALL])\n",
        "    total_all = total_count[ALL]\n",
        "    P = word_count/total_all\n",
        "    P_word[word] = P\n",
        "df_p_word = pd.DataFrame(list(P_word.items()),columns = ['Word','Probability(Word)'])     \n",
        "print(df_p_word) "
      ],
      "id": "9dac104b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0796e77b"
      },
      "source": [
        "P_conditional_pos_word = {}\n",
        "\n",
        "for word in Dict:\n",
        "    word_count = int(Dict[word][POS])\n",
        "    total = total_count[POS]\n",
        "    P = word_count/total\n",
        "    P_conditional_pos_word[word.lower()] = P\n",
        "df_p_conditional_pos_word = pd.DataFrame(list(P_conditional_pos_word.items()),columns = ['Word','Probability(Word|Positive)'])      \n",
        "print(df_p_conditional_pos_word)            "
      ],
      "id": "0796e77b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "288e3cca"
      },
      "source": [
        "P_conditional_neg_word = {}\n",
        "\n",
        "for word in Dict:\n",
        "    word_count_n = int(Dict[word][NEG])\n",
        "    total_n = total_count[NEG]\n",
        "    P_n = word_count_n/total_n\n",
        "    P_conditional_neg_word[word.lower()] = P_n\n",
        "df_p_conditional_neg_word = pd.DataFrame(list(P_conditional_pos_word.items()),columns = ['Word','Probability(Word|Negative)'])     \n",
        "print(df_p_conditional_neg_word)         "
      ],
      "id": "288e3cca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dad2efc1"
      },
      "source": [
        "\n",
        "k=5\n",
        "\n",
        "folds=validation_data \n",
        "#print(validation_data)\n",
        "fold_size= int((len(validation_data))/5)\n",
        "#print(fold_size)\n",
        "# for j in range(k):\n",
        "#     fold=[]\n",
        "#     for i in range(fold_size):\n",
        "#         random_index=randrange(50)\n",
        "#         if len(validation_data)>0:\n",
        "#             #print(validation_data[random_index][0])\n",
        "#             fold.append(validation_data[random_index][0])\n",
        "#             #fold.append(random.choice(validation_data))\n",
        "#             #print(fold)\n",
        "#             #fold.append(newData_validate.pop(random_index))\n",
        "#             #print(i)\n",
        "#     folds.append(fold)\n",
        "#     #print(folds)\n",
        "\n",
        "train_fold = folds[:450]\n",
        "test_fold = folds[450:]\n",
        "\n",
        "\n",
        "\n",
        "# fold_count = 0    \n",
        "# for fold in folds:\n",
        "#     if(fold_count == 4):\n",
        "#         test_fold = fold\n",
        "#     else:\n",
        "#         train_fold = train_fold + fold\n",
        "#     fold_count+=1\n",
        "    #train_fold = folds\n",
        "    #train_fold.remove(fold)\n",
        "    #train_fold = sum(train_fold, [])\n",
        "    #test_fold = fold\n",
        "#print(train_fold)\n",
        "   \n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "dad2efc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00961572"
      },
      "source": [
        "def predict(review, vocabulary, label_count, lam):\n",
        "\n",
        "    probability = []\n",
        "    \n",
        "\n",
        "    for label in[0,1]:\n",
        "        prob = 1.0\n",
        "        counter = 0\n",
        "        for row in review:\n",
        "            if(counter%2==0):\n",
        "                sentarray = row.lower().split()\n",
        "                for word in sentarray:\n",
        "                    #print(word)\n",
        "                    \n",
        "                    if word not in vocabulary:\n",
        "                        #print(\"not found\")\n",
        "                        continue\n",
        "                    if lam == 0 and vocabulary[word][label]==0:\n",
        "                        prob = 0\n",
        "                        #print(\"zero\")\n",
        "                        break\n",
        "                    #word_count_value = vocabulary[word][label]\n",
        "                    prob = round(prob*((vocabulary[word][label]+lam) / (label_count[label]+(lam*len(vocabulary)))),5)\n",
        "                     \n",
        "                counter+=1\n",
        "        #print(prob)        \n",
        "        probability.append(prob)  \n",
        "        #probability[label] = prob[label]*(label_count[label]/label_count[ALL])\n",
        "    #print(probability[0],probability[1])\n",
        "    #print(probability[NEG],probability[POS])\n",
        "    return 0 if probability[NEG]>probability[POS] else 1\n",
        "   "
      ],
      "id": "00961572",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee77582d"
      },
      "source": [
        "l = [0,1]\n",
        "\n",
        "#lam=0   \n",
        "\n",
        "occurrence_shreshold = 5\n",
        "\n",
        "new_train_fold = []\n",
        "\n",
        "\n",
        "                    \n",
        "            \n",
        "#print(new_train_fold)\n",
        "\n",
        "\n",
        "voca = word_vocabulary(train_fold, occurrence_shreshold)\n",
        "\n",
        "label_count = count_label(train_fold)\n",
        "\n",
        "for lam in l:\n",
        "    accuracy = []\n",
        "    correct = 0\n",
        "    for review in test_fold:\n",
        "        flag1 = predict(review, voca, label_count, lam)\n",
        "        #print(flag1)\n",
        "        flag2 = int(review[1])\n",
        "        #print(flag1,flag2)\n",
        "        if(flag1 == flag2):\n",
        "            correct+=1\n",
        "    accuracy.append(correct/len(test_fold))\n",
        "    if(lam==0):\n",
        "        print(\"Accuracy without smoothing is : \",accuracy)\n",
        "    else:\n",
        "        print(\"\\nAccuracy with smoothing is : \",accuracy)"
      ],
      "id": "ee77582d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afca768c"
      },
      "source": [
        "def count_all_pos_Neg_data(data):\n",
        "    label_count=[0,0, len(data)] # [neg_count, pos_count, all_count]\n",
        "    for row in data:\n",
        "        if row[1]==0:\n",
        "            label_count[NEG]+=1\n",
        "            #print(label_count[NEG])\n",
        "        else:\n",
        "            label_count[POS]+=1  \n",
        "    #print(label_count)\n",
        "    return label_count"
      ],
      "id": "afca768c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d29104e1"
      },
      "source": [
        "def Prob_by_Sentiment(data,word,senti):\n",
        "    countarr=count_all_pos_Neg_data(data)\n",
        "    finalarr=[]\n",
        "    count=0\n",
        "    count_conditional=0\n",
        "    if(senti==0):\n",
        "        count=countarr[1]\n",
        "        snt = 0\n",
        "    else:\n",
        "        count=countarr[0]\n",
        "        snt = 1\n",
        "    \n",
        "    #print(count)\n",
        "    for row in data:\n",
        "        if(row[1]==snt):\n",
        "            sentencearr=row[0].lower().split()\n",
        "            if word.lower() in sentencearr:\n",
        "                count_conditional+=1\n",
        "    finalarr.append(count_conditional)\n",
        "    finalarr.append(countarr[0])\n",
        "    finalarr.append(countarr[1])\n",
        "    if(senti==0):\n",
        "        cond_prob=count_conditional/countarr[0]\n",
        "    else:\n",
        "        cond_prob=count_conditional/countarr[1]\n",
        "    #print(cond_prob)    \n",
        "    return(cond_prob)"
      ],
      "id": "d29104e1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f83df0f"
      },
      "source": [
        "def prob_word(data,word):\n",
        "    count=0\n",
        "    for sent in data:\n",
        "        sentarr=sent[0].lower().split()\n",
        "        if word.lower() in sentarr:\n",
        "            count+=1\n",
        "    prob=(count)/len(data)\n",
        "    #print(prob)\n",
        "    return prob"
      ],
      "id": "8f83df0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d783ce9"
      },
      "source": [
        "def get_top_10(data, occurrence_shreshold):\n",
        "    vocabulary = word_vocabulary(data, occurrence_shreshold)\n",
        "    label_count = count_all_pos_Neg_data(data)\n",
        "\n",
        "    word_predict = {}\n",
        "\n",
        "    for row in data:\n",
        "        for word in row[0].split():             \n",
        "            #print(word)\n",
        "            if word not in vocabulary: continue\n",
        "            neg_prob = (Prob_by_Sentiment(data,word, NEG) * (label_count[NEG]/label_count[ALL]))/ prob_word(data,word)\n",
        "            #print(neg_prob)\n",
        "            pos_prob = (Prob_by_Sentiment(data,word,POS) * (label_count[POS]/label_count[ALL]))/ prob_word(data,word)\n",
        "            #print(neg_prob,pos_prob)\n",
        "            pred = NEG if neg_prob > pos_prob else POS\n",
        "            if word not in word_predict:\n",
        "                word_predict[word] = [0,0,0]\n",
        "            word_predict[word][ALL] += 1\n",
        "            \n",
        "            if pred == int(row[1]):  \n",
        "                word_predict[word][pred] += 1\n",
        "            \n",
        "    top_neg_lst = sorted(word_predict, key = lambda x: word_predict[x][NEG]/word_predict[x][ALL], reverse=True) [:10]\n",
        "    top_pos_lst = sorted(word_predict, key = lambda x: word_predict[x][POS]/word_predict[x][ALL], reverse=True)[:10]\n",
        "    finalarr=[]\n",
        "    finalarr.append(top_neg_lst)\n",
        "    finalarr.append(top_pos_lst)\n",
        "    return finalarr"
      ],
      "id": "5d783ce9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c00b5017"
      },
      "source": [
        "arr = get_top_10(validation_data, 5)\n",
        "print(\"Top 10 words that predict negative class are as follows: \\n\", arr[0])\n",
        "print(\"\\nTop 10 words that predict positive class are as follows: \\n\", arr[1])"
      ],
      "id": "c00b5017",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ce00711"
      },
      "source": [
        ""
      ],
      "id": "0ce00711",
      "execution_count": null,
      "outputs": []
    }
  ]
}